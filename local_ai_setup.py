#!/usr/bin/env python3
"""
KAREL Network Dashboard - Local AI Setup
Ollama ve diÄŸer local model seÃ§enekleri
"""

import subprocess
import requests
import json
import os
from typing import Dict, Any, Optional
import platform

class LocalAISetup:
    def __init__(self):
        self.os_type = platform.system()
        self.ollama_url = "http://localhost:11434"
        
    def check_ollama_installed(self) -> bool:
        """Ollama'nÄ±n kurulu olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags")
            return response.status_code == 200
        except:
            return False
    
    def install_ollama(self):
        """Ollama kurulum talimatlarÄ±"""
        print("ğŸ¤– Ollama Kurulumu")
        print("-" * 50)
        
        if self.os_type == "Windows":
            print("""
Windows iÃ§in Ollama kurulumu:

1. TarayÄ±cÄ±dan indirin:
   https://ollama.ai/download/windows
   
2. OllamaSetup.exe dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n

3. Kurulum tamamlandÄ±ktan sonra PowerShell'de:
   ollama --version
   
4. Ollama'yÄ± baÅŸlatÄ±n:
   ollama serve
""")
        elif self.os_type == "Linux":
            print("""
Linux iÃ§in Ollama kurulumu:

curl -fsSL https://ollama.ai/install.sh | sh
""")
        elif self.os_type == "Darwin":  # macOS
            print("""
macOS iÃ§in Ollama kurulumu:

brew install ollama
""")
    
    def download_models(self):
        """Ã–nerilen modelleri indir"""
        models = [
            {
                "name": "deepseek-r1:1.5b",
                "size": "1GB",
                "description": "DeepSeek R1 1.5B - HÄ±zlÄ± ve hafif"
            },
            {
                "name": "deepseek-r1:7b",
                "size": "4GB",
                "description": "DeepSeek R1 7B - Dengeli performans"
            },
            {
                "name": "llama3.2:3b",
                "size": "2GB",
                "description": "Llama 3.2 3B - Alternatif model"
            },
            {
                "name": "qwen2.5:3b",
                "size": "2GB",
                "description": "Qwen 2.5 3B - TÃ¼rkÃ§e desteÄŸi iyi"
            }
        ]
        
        print("\nğŸ“¦ Ä°ndirilebilecek Modeller:")
        for i, model in enumerate(models, 1):
            print(f"\n{i}. {model['name']}")
            print(f"   Boyut: {model['size']}")
            print(f"   AÃ§Ä±klama: {model['description']}")
            print(f"   Ä°ndirme komutu: ollama pull {model['name']}")
    
    def test_model(self, model_name: str = "deepseek-r1:1.5b"):
        """Model testi"""
        print(f"\nğŸ§ª {model_name} Model Testi")
        print("-" * 50)
        
        test_prompt = """
        Fiber optik arÄ±za veritabanÄ± analizi:
        - Toplam 3 arÄ±za kaydÄ±
        - HAGS aÅŸÄ±m oranÄ±: %33.3
        - Ã‡Ã¶zÃ¼m oranÄ±: %66.7
        
        Bu verilere gÃ¶re Ã¶nerileriniz nelerdir?
        """
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model_name,
                    "prompt": test_prompt,
                    "stream": False
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… Model YanÄ±tÄ±:")
                print(result.get("response", "YanÄ±t alÄ±namadÄ±"))
            else:
                print(f"âŒ Hata: {response.status_code}")
                print("Model indirilmemiÅŸ olabilir. Ã–nce: ollama pull", model_name)
        except Exception as e:
            print(f"âŒ BaÄŸlantÄ± hatasÄ±: {str(e)}")
            print("Ollama servisinin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: ollama serve")


class FiberArizaLocalAI:
    """Fiber arÄ±za analizi iÃ§in local AI"""
    
    def __init__(self, model_name: str = "deepseek-r1:7b"):  # 7b varsayÄ±lan
        self.model_name = model_name
        self.ollama_url = "http://localhost:11434"
        self.context_loaded = False
        self.context = ""
        
    def load_context(self):
        """VeritabanÄ± baÄŸlamÄ±nÄ± yÃ¼kle"""
        try:
            # Analiz sonuÃ§larÄ±nÄ± yÃ¼kle
            with open('ml_data/database_analysis.json', 'r', encoding='utf-8') as f:
                analysis = json.load(f)
            
            # Training verilerini yÃ¼kle (ilk 10 Ã¶rnek)
            training_examples = []
            with open('ml_data/training_data.jsonl', 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= 10:  # Sadece ilk 10 Ã¶rnek
                        break
                    training_examples.append(json.loads(line))
            
            # Context oluÅŸtur
            self.context = f"""
Sen TÃ¼rkiye fiber optik altyapÄ± uzmanÄ±sÄ±n. KAREL Network Dashboard veritabanÄ±nÄ± analiz ediyorsun.

## VeritabanÄ± Ã–zeti:
- Toplam fiber arÄ±za kaydÄ±: {analysis['table_analysis']['fiber_ariza']['row_count']}
- HAGS aÅŸÄ±m oranÄ±: %{analysis['table_analysis']['fiber_ariza']['special_analysis']['hags_stats']['hags_percentage']:.1f}
- Ã‡Ã¶zÃ¼m oranÄ±: %{analysis['table_analysis']['fiber_ariza']['special_analysis']['solution_stats']['solution_rate']:.1f}
- Koordinat kapsama: %{analysis['table_analysis']['fiber_ariza']['special_analysis']['coordinate_stats']['coordinate_coverage']:.1f}

## En SÄ±k ArÄ±za Nedenleri:
"""
            root_causes = analysis['table_analysis']['fiber_ariza']['special_analysis'].get('root_cause_distribution', {})
            for cause, count in list(root_causes.items())[:5]:
                self.context += f"- {cause}: {count} kez\n"
            
            self.context += "\n## Ã–rnek ArÄ±za KayÄ±tlarÄ±:\n"
            for example in training_examples[:3]:
                user_msg = example['messages'][1]['content']
                self.context += f"- {user_msg}\n"
            
            self.context_loaded = True
            print("âœ… Context yÃ¼klendi")
            
        except Exception as e:
            print(f"âŒ Context yÃ¼kleme hatasÄ±: {str(e)}")
            self.context = "Fiber optik arÄ±za takip sistemi uzmanÄ±sÄ±n."
    
    def analyze(self, prompt: str, temperature: float = 0.7) -> str:
        """Analiz yap"""
        if not self.context_loaded:
            self.load_context()
        
        full_prompt = f"{self.context}\n\nKullanÄ±cÄ± Sorusu: {prompt}\n\nYanÄ±t:"
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": full_prompt,
                    "temperature": temperature,
                    "stream": False
                }
            )
            
            if response.status_code == 200:
                return response.json().get("response", "YanÄ±t alÄ±namadÄ±")
            else:
                return f"Hata: {response.status_code}"
        except Exception as e:
            return f"BaÄŸlantÄ± hatasÄ±: {str(e)}"
    
    def chat_interactive(self):
        """Ä°nteraktif sohbet modu"""
        print(f"\nğŸ’¬ {self.model_name} ile Fiber ArÄ±za Analizi")
        print("(Ã‡Ä±kmak iÃ§in 'exit' yazÄ±n)")
        print("-" * 50)
        
        self.load_context()
        
        while True:
            user_input = input("\nğŸ‘¤ Soru: ")
            
            if user_input.lower() in ['exit', 'quit', 'Ã§Ä±kÄ±ÅŸ']:
                print("ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
                break
            
            print("\nğŸ¤– Analiz ediliyor...")
            response = self.analyze(user_input)
            print(f"\nğŸ’¡ YanÄ±t:\n{response}")
    
    def generate_report(self) -> str:
        """Otomatik rapor Ã¼ret"""
        report_prompt = """
        LÃ¼tfen aÅŸaÄŸÄ±daki baÅŸlÄ±klarda detaylÄ± bir analiz raporu hazÄ±rla:
        
        1. Mevcut Durum Ã–zeti
        2. Kritik Bulgular
        3. Risk Analizi
        4. Ã‡Ã¶zÃ¼m Ã–nerileri
        5. Ã–ncelikli Aksiyon PlanÄ±
        
        Raporu TÃ¼rkÃ§e hazÄ±rla ve somut Ã¶neriler sun.
        """
        
        return self.analyze(report_prompt, temperature=0.3)  # Daha tutarlÄ± Ã§Ä±ktÄ± iÃ§in dÃ¼ÅŸÃ¼k temperature


# Ã–rnek kullanÄ±m scripti
def main():
    print("ğŸš€ KAREL Network Dashboard - Local AI Kurulumu")
    print("=" * 50)
    
    # Setup kontrolÃ¼
    setup = LocalAISetup()
    
    if not setup.check_ollama_installed():
        print("âš ï¸  Ollama kurulu deÄŸil!")
        setup.install_ollama()
        return
    
    print("âœ… Ollama kurulu ve Ã§alÄ±ÅŸÄ±yor!")
    
    # Model seÃ§enekleri
    setup.download_models()
    
    # KullanÄ±cÄ± seÃ§imi
    print("\n" + "=" * 50)
    choice = input("\nNe yapmak istersiniz?\n1. Model indir\n2. Model test et\n3. Ä°nteraktif analiz\n4. Otomatik rapor\n\nSeÃ§im (1-4): ")
    
    if choice == "1":
        model = input("\nModel adÄ± (Ã¶rn: deepseek-r1:1.5b): ")
        print(f"\nÄ°ndirmek iÃ§in: ollama pull {model}")
        
    elif choice == "2":
        model = "deepseek-r1:7b"  # Otomatik 7b
        setup.test_model(model)
        
    elif choice == "3":
        model = "deepseek-r1:7b"  # Otomatik 7b
        ai = FiberArizaLocalAI(model)
        ai.chat_interactive()
        
    elif choice == "4":
        model = "deepseek-r1:7b"  # Otomatik 7b
        ai = FiberArizaLocalAI(model)
        print("\nğŸ“Š Rapor hazÄ±rlanÄ±yor...")
        report = ai.generate_report()
        print(f"\nğŸ“„ RAPOR:\n{report}")
        
        # Raporu kaydet
        with open('ml_data/ai_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        print("\nâœ… Rapor 'ml_data/ai_report.txt' dosyasÄ±na kaydedildi")


if __name__ == "__main__":
    main()