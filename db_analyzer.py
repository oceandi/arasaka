#!/usr/bin/env python3
"""
KAREL Network Dashboard - AI/ML Veri HazÄ±rlama ModÃ¼lÃ¼
Bu script SQLite veritabanÄ±nÄ± okur ve AI modellerinin anlayabileceÄŸi formatlara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
"""

import sqlite3
import pandas as pd
import json
import numpy as np
from datetime import datetime
import os
from typing import Dict, List, Any

class FiberArizaAnalyzer:
    def __init__(self, db_path: str = 'instance/fiberariza.db'):
        """
        VeritabanÄ± analiz sÄ±nÄ±fÄ±
        
        Args:
            db_path: SQLite veritabanÄ± dosya yolu
        """
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.tables = self._get_tables()
        
    def _get_tables(self) -> List[str]:
        """VeritabanÄ±ndaki tÃ¼m tablolarÄ± listele"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        return [table[0] for table in cursor.fetchall() if not table[0].startswith('sqlite_')]
    
    def analyze_database(self) -> Dict[str, Any]:
        """VeritabanÄ± genel analizi"""
        analysis = {
            'database_info': {
                'path': self.db_path,
                'size_mb': os.path.getsize(self.db_path) / (1024 * 1024),
                'tables': self.tables,
                'analysis_date': datetime.now().isoformat()
            },
            'table_analysis': {}
        }
        
        for table in self.tables:
            analysis['table_analysis'][table] = self._analyze_table(table)
        
        return analysis
    
    def _analyze_table(self, table_name: str) -> Dict[str, Any]:
        """Tek bir tabloyu analiz et"""
        df = pd.read_sql_query(f"SELECT * FROM {table_name}", self.conn)
        
        analysis = {
            'row_count': len(df),
            'column_count': len(df.columns),
            'columns': list(df.columns),
            'data_types': df.dtypes.astype(str).to_dict(),
            'null_counts': df.isnull().sum().to_dict(),
            'unique_counts': {col: df[col].nunique() for col in df.columns},
            'sample_data': df.head(5).to_dict(orient='records')
        }
        
        # Ã–zel analizler
        if table_name == 'fiber_ariza':
            analysis['special_analysis'] = self._analyze_fiber_ariza(df)
        
        return analysis
    
    def _analyze_fiber_ariza(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Fiber arÄ±za tablosu iÃ§in Ã¶zel analizler"""
        analysis = {}
        
        # BÃ¶lgesel daÄŸÄ±lÄ±m
        if 'bolge' in df.columns:
            analysis['bolge_distribution'] = df['bolge'].value_counts().to_dict()
        
        # HAGS analizi
        if 'hags_asildi_mi' in df.columns:
            analysis['hags_stats'] = {
                'total': len(df),
                'hags_exceeded': len(df[df['hags_asildi_mi'] == 'Evet']),
                'hags_percentage': (len(df[df['hags_asildi_mi'] == 'Evet']) / len(df) * 100) if len(df) > 0 else 0
            }
        
        # KalÄ±cÄ± Ã§Ã¶zÃ¼m analizi
        if 'kalici_cozum' in df.columns:
            analysis['solution_stats'] = {
                'solved': len(df[df['kalici_cozum'] == 'Evet']),
                'unsolved': len(df[df['kalici_cozum'] != 'Evet']),
                'solution_rate': (len(df[df['kalici_cozum'] == 'Evet']) / len(df) * 100) if len(df) > 0 else 0
            }
        
        # KoordinatlÄ± arÄ±za analizi
        if 'kordinat_a' in df.columns and 'kordinat_b' in df.columns:
            coords_filled = df[(df['kordinat_a'].notna()) & (df['kordinat_b'].notna())]
            analysis['coordinate_stats'] = {
                'with_coordinates': len(coords_filled),
                'without_coordinates': len(df) - len(coords_filled),
                'coordinate_coverage': (len(coords_filled) / len(df) * 100) if len(df) > 0 else 0
            }
        
        # ArÄ±za sÃ¼releri analizi
        if 'ariza_baslangic' in df.columns and 'ariza_bitis' in df.columns:
            df['ariza_baslangic'] = pd.to_datetime(df['ariza_baslangic'], errors='coerce')
            df['ariza_bitis'] = pd.to_datetime(df['ariza_bitis'], errors='coerce')
            
            valid_dates = df[(df['ariza_baslangic'].notna()) & (df['ariza_bitis'].notna())]
            if len(valid_dates) > 0:
                valid_dates['duration_hours'] = (valid_dates['ariza_bitis'] - valid_dates['ariza_baslangic']).dt.total_seconds() / 3600
                analysis['duration_stats'] = {
                    'avg_duration_hours': valid_dates['duration_hours'].mean(),
                    'max_duration_hours': valid_dates['duration_hours'].max(),
                    'min_duration_hours': valid_dates['duration_hours'].min(),
                    'median_duration_hours': valid_dates['duration_hours'].median()
                }
        
        # KÃ¶k neden analizi
        if 'ariza_kok_neden' in df.columns:
            analysis['root_cause_distribution'] = df['ariza_kok_neden'].value_counts().head(10).to_dict()
        
        return analysis
    
    def export_for_ml(self, output_dir: str = 'ml_data') -> None:
        """ML modelleri iÃ§in veriyi export et"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Her tablo iÃ§in CSV export
        for table in self.tables:
            df = pd.read_sql_query(f"SELECT * FROM {table}", self.conn)
            df.to_csv(f"{output_dir}/{table}.csv", index=False)
            print(f"âœ… {table}.csv exported ({len(df)} rows)")
        
        # Analiz sonuÃ§larÄ±nÄ± JSON olarak kaydet
        analysis = self.analyze_database()
        with open(f"{output_dir}/database_analysis.json", 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        print("âœ… database_analysis.json exported")
        
        # ML iÃ§in Ã¶zel hazÄ±rlanmÄ±ÅŸ veri
        self._prepare_ml_dataset(output_dir)
    
    def _prepare_ml_dataset(self, output_dir: str) -> None:
        """ML modelleri iÃ§in Ã¶zel veri hazÄ±rla"""
        # Fiber arÄ±za verisi iÃ§in feature engineering
        df = pd.read_sql_query("SELECT * FROM fiber_ariza", self.conn)
        
        if len(df) > 0:
            # Feature engineering
            features = pd.DataFrame()
            
            # Kategorik deÄŸiÅŸkenleri encode et
            if 'bolge' in df.columns:
                features['bolge_encoded'] = pd.Categorical(df['bolge']).codes
            
            if 'hags_asildi_mi' in df.columns:
                features['hags_exceeded'] = (df['hags_asildi_mi'] == 'Evet').astype(int)
            
            if 'kalici_cozum' in df.columns:
                features['is_solved'] = (df['kalici_cozum'] == 'Evet').astype(int)
            
            # Koordinat features
            if 'kordinat_a' in df.columns and 'kordinat_b' in df.columns:
                features['has_coordinates'] = ((df['kordinat_a'].notna()) & (df['kordinat_b'].notna())).astype(int)
                
                # KoordinatlarÄ± float'a Ã§evir
                df['kordinat_a'] = pd.to_numeric(df['kordinat_a'].astype(str).str.replace(',', '.'), errors='coerce')
                df['kordinat_b'] = pd.to_numeric(df['kordinat_b'].astype(str).str.replace(',', '.'), errors='coerce')
                
                features['latitude'] = df['kordinat_a'].fillna(0)
                features['longitude'] = df['kordinat_b'].fillna(0)
            
            # Zaman features
            if 'ariza_baslangic' in df.columns:
                df['ariza_baslangic'] = pd.to_datetime(df['ariza_baslangic'], errors='coerce')
                features['start_hour'] = df['ariza_baslangic'].dt.hour
                features['start_day_of_week'] = df['ariza_baslangic'].dt.dayofweek
                features['start_month'] = df['ariza_baslangic'].dt.month
            
            # ML-ready dataset
            ml_dataset = pd.concat([features, df[['bulten_no', 'il', 'ariza_kok_neden']]], axis=1)
            ml_dataset.to_csv(f"{output_dir}/ml_ready_dataset.csv", index=False)
            print("âœ… ml_ready_dataset.csv exported")
            
            # Text data for NLP
            text_data = []
            for _, row in df.iterrows():
                text_entry = {
                    'bulten_no': row.get('bulten_no', ''),
                    'description': f"{row.get('guzergah', '')} {row.get('lokasyon', '')} {row.get('aciklama', '')}",
                    'root_cause': row.get('ariza_kok_neden', ''),
                    'consolidated_cause': row.get('ariza_konsolide', ''),
                    'is_solved': row.get('kalici_cozum', '') == 'Evet'
                }
                text_data.append(text_entry)
            
            with open(f"{output_dir}/text_data_for_nlp.json", 'w', encoding='utf-8') as f:
                json.dump(text_data, f, ensure_ascii=False, indent=2)
            print("âœ… text_data_for_nlp.json exported")
    
    def generate_insights_prompt(self) -> str:
        """R1 modeli iÃ§in prompt oluÅŸtur"""
        analysis = self.analyze_database()
        
        prompt = f"""
# Fiber Optik ArÄ±za VeritabanÄ± Analizi

## VeritabanÄ± Ã–zeti:
- Toplam tablo sayÄ±sÄ±: {len(self.tables)}
- VeritabanÄ± boyutu: {analysis['database_info']['size_mb']:.2f} MB

## Fiber ArÄ±za Tablosu DetaylarÄ±:
"""
        
        if 'fiber_ariza' in analysis['table_analysis']:
            fa = analysis['table_analysis']['fiber_ariza']
            prompt += f"""
- Toplam arÄ±za kaydÄ±: {fa['row_count']}
- SÃ¼tun sayÄ±sÄ±: {fa['column_count']}
"""
            
            if 'special_analysis' in fa:
                sa = fa['special_analysis']
                
                if 'hags_stats' in sa:
                    prompt += f"""
### HAGS Analizi:
- HAGS aÅŸan arÄ±za sayÄ±sÄ±: {sa['hags_stats']['hags_exceeded']} ({sa['hags_stats']['hags_percentage']:.1f}%)
"""
                
                if 'solution_stats' in sa:
                    prompt += f"""
### Ã‡Ã¶zÃ¼m Durumu:
- Ã‡Ã¶zÃ¼len arÄ±zalar: {sa['solution_stats']['solved']} ({sa['solution_stats']['solution_rate']:.1f}%)
- Ã‡Ã¶zÃ¼lmeyen arÄ±zalar: {sa['solution_stats']['unsolved']}
"""
                
                if 'coordinate_stats' in sa:
                    prompt += f"""
### Koordinat Kapsama:
- KoordinatlÄ± arÄ±zalar: {sa['coordinate_stats']['with_coordinates']} ({sa['coordinate_stats']['coordinate_coverage']:.1f}%)
"""
                
                if 'duration_stats' in sa:
                    prompt += f"""
### ArÄ±za SÃ¼re Ä°statistikleri:
- Ortalama arÄ±za sÃ¼resi: {sa['duration_stats']['avg_duration_hours']:.1f} saat
- En uzun arÄ±za: {sa['duration_stats']['max_duration_hours']:.1f} saat
- En kÄ±sa arÄ±za: {sa['duration_stats']['min_duration_hours']:.1f} saat
"""
                
                if 'root_cause_distribution' in sa:
                    prompt += "\n### En SÄ±k ArÄ±za Nedenleri:\n"
                    for cause, count in list(sa['root_cause_distribution'].items())[:5]:
                        prompt += f"- {cause}: {count} kez\n"
        
        prompt += """
## Analiz SorularÄ±:
1. Bu verilere gÃ¶re en kritik arÄ±za bÃ¶lgeleri hangileri?
2. HAGS aÅŸÄ±m oranÄ±nÄ± azaltmak iÃ§in hangi Ã¶nlemler alÄ±nabilir?
3. Ã‡Ã¶zÃ¼lmeyen arÄ±zalarÄ±n ortak Ã¶zellikleri neler?
4. ArÄ±za sÃ¼relerini kÄ±saltmak iÃ§in ne Ã¶nerirsiniz?
5. Gelecekteki potansiyel arÄ±za bÃ¶lgeleri tahmin edilebilir mi?

LÃ¼tfen bu verileri analiz ederek TÃ¼rkiye fiber altyapÄ±sÄ± iÃ§in Ã¶neriler sunun.
"""
        
        return prompt
    
    def close(self):
        """VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kapat"""
        self.conn.close()


# CLI kullanÄ±mÄ±
if __name__ == "__main__":
    print("ğŸ” KAREL Network Dashboard - AI/ML Veri HazÄ±rlama")
    print("-" * 50)
    
    # Analyzer oluÅŸtur
    analyzer = FiberArizaAnalyzer()
    
    # VeritabanÄ± analizi
    print("\nğŸ“Š VeritabanÄ± Analizi BaÅŸlÄ±yor...")
    analysis = analyzer.analyze_database()
    
    print(f"\nâœ… Toplam {len(analyzer.tables)} tablo bulundu:")
    for table in analyzer.tables:
        row_count = analysis['table_analysis'][table]['row_count']
        print(f"  - {table}: {row_count} kayÄ±t")
    
    # ML export
    print("\nğŸ“ ML Verileri Export Ediliyor...")
    analyzer.export_for_ml()
    
    # R1 prompt oluÅŸtur
    print("\nğŸ¤– R1 Modeli Ä°Ã§in Prompt OluÅŸturuluyor...")
    prompt = analyzer.generate_insights_prompt()
    
    with open('ml_data/r1_analysis_prompt.txt', 'w', encoding='utf-8') as f:
        f.write(prompt)
    print("âœ… r1_analysis_prompt.txt kaydedildi")
    
    # Ã–zet istatistikler
    if 'fiber_ariza' in analysis['table_analysis']:
        fa = analysis['table_analysis']['fiber_ariza']['special_analysis']
        print("\nğŸ“ˆ Ã–zet Ä°statistikler:")
        print(f"  - HAGS AÅŸÄ±m OranÄ±: {fa.get('hags_stats', {}).get('hags_percentage', 0):.1f}%")
        print(f"  - Ã‡Ã¶zÃ¼m OranÄ±: {fa.get('solution_stats', {}).get('solution_rate', 0):.1f}%")
        print(f"  - Koordinat Kapsama: {fa.get('coordinate_stats', {}).get('coordinate_coverage', 0):.1f}%")
    
    analyzer.close()
    print("\nâœ… Analiz tamamlandÄ±! ml_data/ klasÃ¶rÃ¼nÃ¼ kontrol edin.")